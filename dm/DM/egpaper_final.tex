\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{CJK}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Text and Image Data Mining Based on Acquaintance Social Network 2}

\author{Hao Yuan\\
{\tt\small MG1832011}
\and Kuo Chi\\
{\tt\small MF1832026}
\and Yuhua Luo\\
{\tt\small MF1832115}
\and Fan Yu\\
{\tt\small MG1832010}}

\maketitle

\begin{abstract}
    Wechat moods and QQ zone are different from public online communities like Weibo, Zhihu and Baidu tieba for they are built on the basis of friends, relatives and classmates who are familiar with each other.
    Therefore, we call this type of community an ��acquaintance social network�� , which is opposed to a stranger social network.
    In the acquaintance social network, expect for online communication, there often exists more direct offline contact, which makes the communication methods of acquaintance social network and the stranger social network different.
    With the rapid development of the Internet, ordinary users need to establish a good impression in the social network of acquaintances, and send ��moods�� to attract the attention of friends.
    Based on the real data collected in the QQ zone, this paper uses the heat to measure the popularity of each "mood", and thus models and analyzes how to produce more popular "moods" in the acquaintance social network, and proposes a method for constructing user portrait based on text, image and other data published by user in acquaintance social network.

    \keywords{acquaintance social network, natural language processing, image processing, user portrait}

\end{abstract}

\section{Introduction}
    Chinese online communities can be roughly divided into two categories.
    One is the stranger social networks represented by Weibo, Zhihu, and Baidu tieba, in which the contents published by each user can be viewed by most users casually, regardless of whether the users know each other.
    Therefore, the contents are usually extensively beautified and modified, and the authenticity is difficult to judge.
    The other one is the acquaintance social networks ,like Wechat moods and QQ zone, in which the contents published by each user can be viewed by online friends who are classmates, colleagues, relatives and friends of the user.
    Therefore, the contents in such communities are closer to the user��s real thoughts and personality.
    By constructing user portraits through these contents, we can distinguish user groups better for the purpose of precision marketing.

    In the meanwhile, with the rapid development of the Internet, social networks are more closely connected with people's daily communication, and ordinary users also urgently need to create a good image in the acquaintance social network by posting some modified and beautified "moods".
    However, the way to obtain heat in the stranger social network does not work in the acquaintance social network because the users receiving the "moods" usually has real contact and understanding of the user, such as self-portraits that are completely inconsistent with the real person are more likely to cause dislike of acquaintances.

    Therefore, this paper models the relationship between content and popularity generated by acquaintances' social networks through machine learning and other methods, and explores what kind of content is more popular in acquaintance social networks.
    This paper measures the degree of popularity by heat, which is a value based on pageviews, clicks, and comments, which are defined in Concepts and Definition.

    This paper takes the moods in QQ zone as an example to quantify and analyze each mood from the following indicators: content of text in moods, mood of text in moods, quality and beauty of images in moods, content of images in moods and the time when the user post the moods.
    Also, these metrics will also be used to automatically build a user portrait for each user.

\section{Concepts and Definition}

\subsection{Data Source}
    The data are collected from QQ zone from June 10, 2014 to the present and all collected users have clearly understood and agreed to the study.
    The number of moods which are recorded as D is 14,462.
    After data cleaning, every mood is showed in Table~\ref{fields}.

    \setlength{\tabcolsep}{1pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{moods data fields}
    \label{fields}
    \begin{tabular}{lcc}
    \hline\noalign{\smallskip}
     fields & description & type\\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    tid & unique identifier for each mood & string\\
    like\_num & count of approval & int\\
    prd\_num & count of pageviews & int\\
    cmt\_num & count of comments & int\\
    cmt\_total\_num & total number of comments & int\\
    time\_stamp & timestamp when the mood is posted & int\\
    content & textual content of each mood & string\\
    uin\_list & list of users who like this mood & json\\
    cmt\_list & list of users who write comments & json\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}

\subsection{Definition of Heat(E)}
    Let the average number of each mood in D be m, the amount of praise is n, and the pageview amount is h, then the conversion relationship between comment volume and praise amount is a, and the conversion relationship between the amount of praise and the pageview is b:
    \begin{equation}
    a = n / m,
    \end{equation}
    \begin{equation}
    b = h / n,
    \end{equation}
    e is the entropy of each mood:
    \begin{equation}
    e = m_{i} * a * b + n_{i} * b + h_{i},
    \end{equation}
    in which, $m_{i}$, $n_{i}$ and $h_{i}$ are number of comments, amount of approval and amount of pageviews of the $i^{th}$ mood respectively.

    Thus heat of each mood E is defined as the normalized value of entropy e:
    \begin{equation}
    E_{i} = (e_{i}-e_{min})/(e_{max}-e_{min}),
    \end{equation}
    in which $e_{max}$ represents the maximum entropy of the user who produced the $i^{th}$ mood, and $e_{min}$ represents the minimum entropy.

    Thus, under the assumption that the number of friends of each user is changeless, the size of E is independent of the number of friends.

\section{Implement}

\subsection{Text Content Classification Based on RNN}
    Text types in the acquaintances social networks tend to be short, fragmented, and often have special symbols with ambiguous semantics, resulting in complex and diverse semantics.
    In this paper, we have labeled and constructed a data set containing more than 7,000 texts, and used the Recurrent Neural Network model for character level training and learning, and the final F1 reached 0.83.

    \textbf{Construction of Dataset}
    7 text types are established from the randomly sampled 7,230 text data from the original data set D, which is shown in Table~\ref{texttypes}
    \setlength{\tabcolsep}{1pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{text types}
    \label{texttypes}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
     id & type & description & example\\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    1 & \tabincell{c}{Tourism \\and \\Sports} & \tabincell{c}{Including outing, outdoor \\or indoor sports} & \begin{CJK*}{GBK}{song}\tabincell{c}{�����Ĵ����ߣ�\\�����ڲſ�ʼ}\end{CJK*}\\
    2 & \tabincell{c}{Love \\and \\Family} & \tabincell{c}{Anything related to \\love and family, \\including anything that \\mentions girlfriend, \\boyfriend and \\family members} & \begin{CJK*}{GBK}{song}\tabincell{c}{�������Ժ�Ͳ�\\Ҫ����Ҫ��Ů\\Ʊ����Ƭ��} \end{CJK*}\\
    3 & \tabincell{c}{Learning \\and \\Working} & \tabincell{c}{Anything related to \\study or work, \\including anything that \\mentions school, class, \\company, work, \\classmates, \\colleagues} & \begin{CJK*}{GBK}{song} \tabincell{c}{������������\\�����ᵽʲ\\ô�С�ֻҪѡ��\\ѡ�úá�����} \end{CJK*}\\
    4 & \tabincell{c}{advertisement} & \tabincell{c}{Including advertisements \\for likes, requests \\for forwarding, selling \\things, or various \\micro-businesses} & \begin{CJK*}{GBK}{song} \tabincell{c}{�ײ⣬����װ\\�ޣ��������֣�\\���ϼ۸�����\\��Χ����nice��\\����¥����~} \end{CJK*}\\
    5 & \tabincell{c}{Daily Life} & \tabincell{c}{Trivial things in life \\other than those in 1, \\2, 3 and 4, such as \\dressing, eating, \\accommodation, etc} & \begin{CJK*}{GBK}{song} \tabincell{c}{����Բ�ȥ} \end{CJK*}\\
    6 & \tabincell{c}{Others} & \tabincell{c}{The text is too \\short or there is no \\text or the text \\does not contain Chinese} & http://url.cn/WqvxQE\\
    7 & \tabincell{c}{Insights \\on Life} & \tabincell{c}{A remark about life or \\ideals, which is usually \\unintelligible or a text \\with strong emotion} & \begin{CJK*}{GBK}{song} \tabincell{c}{ʧ������µ�\\������������\\�Ĳ���} \end{CJK*}\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
    According to the seven types above, we labeled each of the text and get the data composed like that in Table~\ref{struct}:
    \setlength{\tabcolsep}{40pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{labeled data structure}
    \vspace{0.5cm}
    \label{struct}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
    field & description\\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    label & text type\\
    content & text content\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
    Finally, the number of each type of data is shown in Table~\ref{totalnumber}:
    \setlength{\tabcolsep}{40pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{number of each type}
    \vspace{0.5cm}
    \label{totalnumber}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
    type & count\\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    1 & 440\\
    2 & 229\\
    3 & 1038\\
    4 & 185\\
    5 & 3051\\
    6 & 1510\\
    7 & 777\\
    total & 7230\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
    Because the total number is small, the training set, validation set, and test set are divided according to the ratio of 7:3:3.

    \textbf{Training with RNN}
    The model framework is shown in Figure~\ref{fig:3121}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{3121}
    \end{center}
    \vspace{-0.5cm}
       \caption{model framework}
       \label{fig:3121}
    \end{figure}
    The RNN is implemented by tensorflow and contains two hidden layers, and each layer contains 128 neurons.
    The convergence process of loss function and accuracy is shown in Figure~\ref{fig:3122} and Figure~\ref{fig:3122}:
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{3122}
    \end{center}
    \vspace{-0.5cm}
       \caption{loss function}
       \label{fig:3122}
    \end{figure}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{3123}
    \end{center}
    \vspace{-0.5cm}
       \caption{accuracy}
       \label{fig:3123}
    \end{figure}
    The final test result is shown in Table~\ref{testresult}
    \setlength{\tabcolsep}{4pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{final test result}
    \label{testresult}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
     & precision & recall & f1-score & support \\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    Tourism and Sports & 0.74 & 0.83 & 0.78 & 132\\
    Love and Family & 0.77 & 0.76 & 0.77 & 76\\
    Learning and Working & 0.8 & 0.82 & 0.81 & 308\\
    Advertisement & 0.74 & 0.67 & 0.7 & 55\\
    Daily Life & 0.86 & 0.87 & 0.86 & 919\\
    Others & 0.88 & 0.84 & 0.86 & 401\\
    Insights on Life & 0.74 & 0.73 & 0.74 & 224\\
    avg/total & 0.83 & 0.83 & 0.83 & 2115\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
    The confusion matrix is shown in Table~\ref{matrix}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{confusion matrix}
    \vspace{0.5cm}
    \label{matrix}
    \begin{tabular}{ccccccc}
    109&0&2&0&14&1&6\\
    2&58&2&0&10&2&2\\
    7&3&253&3&31&4&7\\
    3&0&3&37&10&1&1\\
    20&3&45&5&795&29&22\\
    2&6&3&4&30&338&18\\
    4&5&8&1&35&8&163\\
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}

    \textbf{Result Analysis}
    The final f1 is not very good and may be concerned with these two elements: the first one is the sample data set is too small, and the second one is the data set labeling results are not good.
    We only use a single classification to label but the same paragraph may involve multiple aspects.
    If we use a 7-dimensional vector (each dimension uses 0 and 1 to indicate the presence or absence of the classification) instead of a single classification, we may get better results.
    The relationship between text type and E is shown in Figure~\ref{fig:e4}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{e4}
    \end{center}
    \vspace{-0.5cm}
       \caption{relationship between text type and E}
       \label{fig:e4}
    \end{figure}
\subsection{Text sentiment detection Based on Baidu AI open platform}
    In social networks, the emotions contained in texts published by users can greatly affect other users.
    Some users who always publish positive and optimistic trends will create a positive and optimistic personal impression in the minds of people, and vice versa.
    This subsection will classify the emotions of the text to explore the impact of emotion on the heat.

    Baidu AI open platform is a free online AI service launched by Baidu, which can realize natural language processing and image processing functions simply and quickly by downloading its SDK.
    We use the emotional tendency analysis to detect the emotion in the collected text data and classify the emotional type, and the result includes three types: the position type, the negative type and the neutral type.
    The codes of python for detect emotion using the SDK provided by Baidu AI open platform are shown in the Table~\ref{emotioncode}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{python code}
    \vspace{0.5cm}
    \label{emotioncode}
    \begin{tabular}{l}
    \hline
    from aip import AipNlp\\
    import json\\
    client = AipNlp(APP\_ID, API\_KEY, SECRET\_KEY)\\
    text = 'this is test'\\
    result = self.client.sentimentClassify(text)\\
    sentiment = result['items'][0]['sentiment']\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
     The APP\_ID, API\_KEY and SECRET\_KEY here are needed to applied in Baidu Developers Center.

     The relationship between text emotion and E is shown in the Figure~\ref{fig:e2}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{e2}
    \end{center}
    \vspace{-0.5cm}
       \caption{relationship between text emotion and E}
       \label{fig:e2}
    \end{figure}

\subsection{Image quality and aesthetic rating Based on Google NIMA model}
    As an important medium in social networks, images are often richer in information than words.
    A photo with exquisite composition, wonderful colors and superior quality is more attractive than a thousand words.
    In this sebsection, we try to quantify the quality and aesthetics of images to explore the relationship between mood images and heat in social networks.

    Quantification of image quality and aesthetics is a problem in image processing and computer vision.
    Image Quality Assessment processes the pixel-level degradation problems like noise, blur and compression distortion.
    Aesthetic evaluation is the extraction of semantic level features related to emotion and beauty in images.

    Some test photos from the large-scale database for Aesthetic Visual Analysis (AVA) dataset~\cite{Murray12}, as ranked by NIMA~\cite{Talebi18}, are shown in the Figure~\ref{fig:331}.
    Each AVA photo is scored by an average of 200 people in response to photography contests.
    After training, the aesthetic ranking of these photos by NIMA closely matches the mean scores given by human raters.
    We find that NIMA performs equally well on other datasets, with predicted quality scores close to human ratings.
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{331}
    \end{center}
    \vspace{-0.5cm}
       \caption{the AVA dataset}
       \label{fig:331}
    \end{figure}
    This paper is based on the open source code on github and use the pretrained model based on the AVA data set, and implement photo rating of the NIMA system on social networks.
    The relationship between image rating and E is shown in Figure~\ref{fig:e1}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{e1}
    \end{center}
    \vspace{-0.5cm}
       \caption{relationship between image rating and E}
       \label{fig:e1}
    \end{figure}

\subsection{Object Detection in Image}
    We use Faster R-CNN~\cite{Shaoqing17} to detect objects in images.
    The model we used is trained on MSCOCO, which contains 91 categories of objects.
    We extract the objects detected in an image and use them as a feature of the "mood".
    To use the feature in the clustering, we represent the supercategory exists in the image as 1 and represent it as 0 otherwise.

\subsection{Dynamic Time Classification Generation}
    For most media information that needs to be disseminated, the time when the information is sent is critical because it determines the size of the group that can read the information to some extent.
    This paper attempts to mine the dynamic time of the user to explore the impact of the time of publication mood on the mood heat.
    We divide a day into six time periods as Table~\ref{day}:
    \setlength{\tabcolsep}{20pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{time period division}
    \vspace{0.5cm}
    \label{day}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
    id & name & time period \\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    0 & midnight & 00:00~04:00\\
    1 & dawn & 04:00~08:00\\
    2 & morning & 08:00~12:00\\
    3 & afternoon & 12:00~16:00\\
    4 & Evening & 16:00~20:00\\
    5 & night & 20:00~24:00\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}
    The relationship between the time when the mood is posted and E is shown in Figure~\ref{fig:e3}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{e3}
    \end{center}
    \vspace{-0.5cm}
       \caption{relationship between time and E}
       \label{fig:e3}
    \end{figure}

\subsection{Nonlinear Fitting Based on Xgboost}
    Through the above five steps, data such as text, image, time and the like in the social network are cleaned and converted into corresponding continuous values or discrete values.
    In order to explore the common influence of the above five indicators on heat, this paper uses xgboost-based~\cite{Tianqi16} integrated learning method to fit all data.

    \textbf{Data Source}
    The 14462 data collected from the QQ zone, after a series of data cleaning, transformation and fusion, is finally filtered to 1982 relatively complete data for training.
    And the data is divided the training set and the test set according to the ratio of 7:3.

    \textbf{Results}
    The results of the experiment were evaluated using the root mean square error (RMSE).
    The results are as shown in Table~\ref{rmseresult}
    \setlength{\tabcolsep}{40pt}
    \begin{table}
    \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
    \begin{center}
    \caption{RMSE on training set and test set}
    \vspace{0.5cm}
    \label{rmseresult}
    \begin{tabular}{rcccc}
    \hline\noalign{\smallskip}
    data set & RMSE \\
    \noalign{\smallskip}
    \hline
    \noalign{\smallskip}
    training set & 0.002\\
    test set & 1.997\\
    \hline
    \end{tabular}
    \end{center}
    \end{table}
    \setlength{\tabcolsep}{1.4pt}

    \textbf{Result Analysis}
    The experimental results represent over-fitting, which is related to many factors, for example the data set is too small, the text classification is not accurate enough, the image classification is not accurate enough, etc.
    But the main factor is that it is not enough to understand and extract mood content.
    Due to the complexity of the language itself and the large number of "expression packs" in the online community, information extraction becomes extremely difficult.

\section{Construction of user portraits}

\subsection{User-based Image Construction Based on Statistics}
    In the previous sections, we describe the method of extracting five kinds of effective information from the "moods" in detail.
    These extracted information can be used for the construction of user portraits after some statistics and analysis.
    For example, if the user publishes a dynamic time classification statistical result in the Figure~\ref{fig:411}, it can be found that the user often stays up late.
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{411}
    \end{center}
    \vspace{-0.5cm}
       \caption{The user publishes dynamic time classification statistics and often stays up late}
       \label{fig:411}
    \end{figure}

\subsection{User Image Construction Based on Clustering}
    By modeling and clustering the friends and common groups between friends of the users, we got the following results which is shown in Figure~\ref{fig:421}
    \begin{figure}[t]
    \begin{center}
    \includegraphics[width=\linewidth]{421}
    \end{center}
    \vspace{-0.5cm}
       \caption{friends clustering}
       \label{fig:421}
    \end{figure}

\section{Conclusion}
    This paper mines and analyzes the data in acquaintance social networks through natural language processing and image processing.
    We use a variety of algorithms and models for deep learning and machine learning, involving data acquisition, data cleaning, transformation, fusion, mining and many other processes.
    However, due to limited time, the experimental results are not perfect, and it is difficult to draw valuable information.
    But it still puts forward a variety of ideas and methods for discussing "what kind of content in acquaintances social networks are more popular with friends".
    We hope to continue relevant study in the later period.
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}
